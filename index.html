<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Yoonhwa (Yuna) Jung</title>

    <meta name="author" content="Yuna Jung">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/org_ico.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet_final.css">
  </head>

  <body>
    <div class="container">
      <div>
        <h1>Yoonhwa Jung</h1>
        <p>
          I'm a PhD candidate at the University of Illinois at Urbana-Champaign</a> (UIUC) advised by Prof. <a href="https://cee.illinois.edu/directory/profile/mgolpar">Mani Golparvar-Fard</a> and <a href="https://cs.illinois.edu/about/people/faculty/juliahmr">Julia Hockenmaeir</a>. I'm currently working at <a href="https://www.documentcrunch.com/">Document Crunch</a> AI R&D team, for quickly identifying critical risks and empowering people to understand what’s in their construction contracts.
        </p> 
        <div class="title">
          <p>
            My research focuses on Natural Language Processing (NLP) and Computer Vision, including: 
          </p>
          <ul class="sub-list">
            <li> analyzing construction best practices to generate actionable insights for automated project planning and controls; </li>
            <li> leveraging synergies between lean construction, BIM and Reality modeling for construction production management;</li>
            <li> creating multimodal models for human-interative construction management systems </li>
          </ul>   
        </div>
        <p>
          Simultaneously, my contributions extend to efficiency and effectiveness in various general NLP tasks, where I proposed novel approaches to resolve general problems, yielding a paper accepted to EMNLP 2023 Findings, one under-review ECCV 2024 paper, and preparing one AAAI 2025 paper.
        </p>
      </div>

      <div class="image-container">
        <a href="images/try_profile1.jpg">
            <div class="profile-photo"></div>
        </a>
        <p>
          <a href="mailto:yoonhwa2@illinois.edu">Email</a> &nbsp;/&nbsp;
          <a href="data/Yoonhwa-CV.pdf">CV</a> &nbsp;/&nbsp;
          <a href="https://scholar.google.com/citations?user=3ucRFYgAAAAJ&hl=en&oi=ao">Scholar</a> &nbsp;/&nbsp;
          <a href="https://github.com/joonv2/">Github</a>
       </p>
      </div>
    </div>

    <div class="section">
      <h2>Education</h2>
      <div class="item">
        <div class="title">
          <p>University of Illinois at Urbana-Champaign</p>
          <ul class="sub-list">
            <li>Ph.D. Candidate in Civil Engineering &#x2014; AI in Construction</li>
            <li>Master of Computer Science <i>(GPA: 4.0 / 4.0)</i></li>
            <li>Master of Science in Civil Engineering, Construction Eng. & Mgmt.</li>
          </ul>
        </div>
        <div class="date">
            <p>Aug 2019 - Current</p>
        </div>
      </div>
      <div class="item">
        <div class="title">
          <p>Hanyang University, Seoul, South Korea</p>
          <ul class="sub-list">
            <li>B.S. in Civil Engineering</li>
            <li>High Honors for Academic Achievement (Sep 2015)</li>
          </ul>
        </div>
        <div class="date">
            <p>Mar 2015 - Feb 2019</p>
        </div>
      </div>
    </div>

    <div class="section">
      <h2>Awards & Scholorship</h2>
      <div class="item">
        <div class="title">
          <p>Fellowship, UIUC</p>
        </div>
        <div class="date">
          <p>2024</p>
        </div>
      </div>
      <div class="item">
        <div class="title">
          <p>Scholorship, Korean-American CEPM Association and HanmiGlobal</p>
        </div>
        <div class="date">
          <p>Mar 2024</p>
        </div>
      </div>
      <div class="item">
        <div class="title">
          <p>Grand prize, 4th Industrial Revolution Imagination Contest, Hanyang Univ</p>
        </div>
        <div class="date">
          <p>Nov 2017</p>
        </div>
      </div>
      <div class="item">
        <div class="title">
          <p>Encouragement Prize, Design Construction Competition, KSCE</p>
        </div>
        <div class="date">
          <p>Apr 2016</p>
        </div>
      </div>
      <div class="item">
        <div class="title">
          <p>4-Year Full Scholarship, POSCO</p>
        </div>
        <div class="date">
          <p>Mar 2015 - Dec 2018</p>
        </div>
      </div>
    </div>
    

    <div class="research">
      <h2>Research</h2>
      <p>
        I'm interested in natural language processing, computer vision, multimodal learning, deep learning, generative AI, and human-computer interaction.
        Most of <a href="https://scholar.google.com/citations?user=3ucRFYgAAAAJ&hl=en&oi=ao">my research</a> is about inferring the knowledge (i.e., best practices) of construction planning and controls from textual (e.g., schedules, daily reports, change orders, RFI, etc.) and visual (e.g., images, point clouds, etc.) information. 
        Representative papers are <span class="highlight">highlighted</span>.
      </p>
      <h3>Journal Papers</h3>
      
        <div class="container2">
        <div class="column-img">
          <div class="video-container">
            <video width="100%" height="100%" muted autoplay loop style="position: absolute; top: 0; left: 0; opacity: 1; z-index: -1;">
              <source src="images/VSD_demo.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
            <img src="images/VSD2.jpg" style="width: 100%; display: block;">
          </div>
        </div>
        <div class="column-text">
          <a href="https://github.com/joonv2/VisualSiteDiary">
            <span class="papertitle">VisualSiteDiary: A Detector-Free Vision-Language Transformer Model for Captioning Photologs for Daily Construction Reporting and Image Retrievals</span>
          </a>
          <br><br>
          <strong>Yoonhwa Jung</strong>, Ikhyun Cho, Shun-Hsiang Hsu, Mani Golparvar-Fard
          <br>
          <em>Automation in Construction</em>, 2024
          <br>
          <a href="https://github.com/joonv2/VisualSiteDiary">project page</a> /
          <a href="https://github.com/joonv2/VisualSiteDiary/blob/main/media/VSD_demo.gif">video</a> /
          <a href="https://doi.org/10.1016/j.autcon.2024.105483">Journal paper</a>
          <br><br>
          <p>A Vision Transformer-based image captioning model, VisualSiteDiary, which creates human-readable captions for daily progress and work activity log, and enhances image retrieval tasks. Present a new image-caption pair dataset (VSD dataset) and Demo toward a real-time construction site daily log reporting. Superior-quality captions are generated compared to the state-of-the-art image captioning models.</p>
        </div>
      </div>

      <div class="container2 highlight">
        <div class="column-img">
          <div class="video-container">
            <video width="100%" height="100%" muted autoplay loop style="position: absolute; top: 0; left: 0; opacity: 1; z-index: -1;">
              <img src="images/UniformatBridge_cover.jpg" type="video/jpg">
              Your browser does not support the video tag.
            </video>
            <img src="images/UniformatBridge_cover.jpg" style="width: 100%; display: block;">
          </div>
        </div>
        <div class="column-text">
          <a href="https://doi.org/10.1016/j.autcon.2023.105183">
            <span class="papertitle">UniformatBridge: Transformer language model for mapping construction schedule activities to uniformat categories</span>
          </a>
          <br><br>
          <strong>Yoonhwa Jung</strong>, Julia Hochenmaier, Mani Golparvar-Fard
          <br>
          <em>Automation in Construction</em>, 2024
          <br>
          <a href="https://doi.org/10.1016/j.autcon.2023.105183">Journal paper</a> /
          <a href="https://doi.org/10.1061/9780784485224.006">Conference paper</a>
          <br><br>
          <p>A new NLP transformer model, <span class="small-caps">UniformatBridge</span>, to automatically map schedule activities to ASTM UniFormat classes, embedding construction schedule sequencing knowledge. UniformatBridge serves as a universal identifier enabling automated creation of 4D BIMs and streamlines mapping between schedule, cost and payment application data.</p>
        </div>
      </div>
      <div class="container2">
        <div class="column-img">
          <div class="video-container">
            <video width="100%" height="100%" muted autoplay loop style="position: absolute; top: 0; left: 0; opacity: 1; z-index: -1;">
              <source src="images/schedulerevision2.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
            <img src="images/schedulerevision_cover.png" style="width: 100%; display: block;">
          </div>
        </div>
        <div class="column-text">
          <a href="https://doi.org/10.1016/j.autcon.2023.104896">
            <span class="papertitle">Construction schedule augmentation with implicit dependency constraints and automated generation of lookahead plan revisions</span>
          </a>
          <br><br>
          Fouad Amer, <strong>Yoonhwa Jung</strong>, Mani Golparvar-Fard
          <br>
          <em>Automation in Construction</em>, 2023
          <br>
          <a href="https://doi.org/10.1016/j.autcon.2023.104896">Journal paper</a>
          <br><br>
          <p>A NLP transformer method for deciphering implicit construction dependencies with the role and flexibility of activity relationships. A method for revising lookahead plans based on the flexibility of activity dependencies, mitigating the risk of delays.</p>
        </div>
      </div>

      <div class="container2 highlight">
        <div class="column-img">
          <div class="video-container">
            <img src="images/autoalign_flip1.png" data-hover="images/autoalign_flip2.png">
          </div>
        </div>
        <div class="column-text">
          <a href="https://doi.org/10.1016/j.autcon.2021.103929">
            <span class="papertitle">Transformer machine learning language model for auto-alignment of long-term and short-term plans in construction</span>
          </a>
          <br><br>
          Fouad Amer, <strong>Yoonhwa Jung</strong>, Mani Golparvar-Fard
          <br>
          <em>Automation in Construction</em>, 2021
          <br>
          <a href="https://doi.org/10.1016/j.autcon.2021.103929">Journal paper</a>
          <br><br>
          <p>This paper presents the first attempt to automate linking look-ahead planning tasks to master-schedule activities following an NLP-based multi-stage ranking formulation. Our model employs distance-based matching for candidate generation and a Transformer architecture for final matching.</p>
        </div>
      </div>

      <div class="container2">
        <div class="column-img">
          <div class="video-container">
            <video width="100%" height="100%" muted autoplay loop style="position: absolute; top: 0; left: 0; opacity: 1; z-index: -1;">
              <img src="images/pvpowerpred.png">
              Your browser does not support the video tag.
            </video>
            <img src="images/pvpowerpred.png" style="width: 100%; display: block;">
          </div>
        </div>
        <div class="column-text">
          <a href="https://doi.org/10.1016/j.jclepro.2019.119476">
            <span class="papertitle">Long short-term memory recurrent neural network for modeling temporal patterns in long-term power forecasting for solar PV facilities: Case study of South Korea</span>
          </a>
          <br><br>
          <strong>Yoonhwa Jung</strong>, Jaehoon Jung, Byungil Kim, SangUk Han
          <br>
          <em>Journal of Cleaner Production</em>, 2020
          <br>
          <a href="https://doi.org/10.1016/j.jclepro.2019.119476">Journal paper</a>
          <br><br>
          <p>An LSTM-RNN-based forecasting model is presented for investigation of PV sites. Time series data of spatial and meteorological conditions are considered and this work allows to search and evaluate suitable locations for PV plants in a wide area.</p>
          <br><br>
          <font color="red"><strong>*Cited over 130</strong></font>
        </div>
      </div>
      <br>
      <h3>Peer-reviewed Conference Papers</h3>
      <div class="container2 highlight">
        <div class="column-img">
          <div class="video-container">
            <video width="100%" height="100%" muted autoplay loop style="position: absolute; top: 0; left: 0; opacity: 1; z-index: -1;">
              <source src="images/ConstructVoiceBot.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
            <img src="images/CVB.png" style="width: 100%; display: block;">
          </div>
        </div>
        <div class="column-text">
          <a href="">
            <span class="papertitle">Voice-assisted AI-Chatbot for Construction: Speech-to-Text Deep Learning Transformer trained with Synthetic Datasets</span>
          </a>
          <br><br>
          <strong>Yoonhwa Jung</strong>, Mani Golparvar-Fard
          <br>
          <em>Computing in Civil Engineering</em>, 2024
          <br>
          <a href="">Conference paper</a>
          <br><br>
          <p>This paper presents a Voice-activated Artificial Intelligence (AI) assistant, CONSTRUCTVOICEBOT. Our solution leverages the first domain-specific Speech-to-Text Transformer model, called CON-WHISPER, with a synthetic voice-text dataset from 35 commercial building projects. We delve into practical applications through use cases such as Time and Material reporting, daily construction reporting, quality assurance, and curation of construction workflows.
          <br><br>
          </p>
        </div>
      </div>

      <div class="container2 highlight">
        <div class="column-img">
          <div class="video-container">
            <video width="100%" height="100%" muted autoplay loop style="position: absolute; top: 0; left: 0; opacity: 1; z-index: -1;">
              <img src="images/isarc_2024.png" type="video/mp4">
              Your browser does not support the video tag.
            </video>
            <img src="images/isarc_2024.png" style="width: 100%; display: block;">
          </div>
        </div>
        <div class="column-text">
          <a href="">
            <span class="papertitle">Evaluation of Mapping Computer Vision Segmentation from Reality Capture to Schedule Activities for Construction Monitoring in the Absence of Detailed BIM</span>
          </a>
          <br><br>
          Juan D. Núñez-Morales<sup>&#134;</sup>, <strong>Yoonhwa Jung</strong><sup>&#134;</sup>, Mani Golparvar-Fard
          <br>
          <em>Proceedings of the 41th ISARC</em>, 2024
          <br>
          <a href="">Conference paper</a> (<sup>&#134;</sup>Equal contribution)
          <br><br>
          <p>Built on <a href="https://doi.org/10.1016/j.autcon.2023.105183">UniformatBridge</a>, ASTM Uniformat classification is utilized to map color-coded 3D point clouds aligned with schedule activities without relying on BIM as a baseline. Exemplary results on tied new transformer-based models with few-shot learning are shown.
          <br><br>
          </p>
        </div>
      </div>

      <div class="container2">
        <div class="column-img">
          <div class="video-container">
            <video width="100%" height="100%" muted autoplay loop style="position: absolute; top: 0; left: 0; opacity: 1; z-index: -1;">
              <img src="images/CRC_2024.png">
              Your browser does not support the video tag.
            </video>
            <img src="images/CRC_2024.png" style="width: 100%; display: block;">
          </div>
        </div>
        <div class="column-text">
          <a href="https://doi.org/10.1061/9780784485262.084">
            <span class="papertitle">Bi-Directional Image-to-Text Mapping for NLP-Based Schedule Generation and Computer Vision Progress Monitoring</span>
          </a>
          <br><br>
          Juan D. Núñez-Morales, <strong>Yoonhwa Jung</strong>, Mani Golparvar-Fard
          <br>
          <em>Construction Research Congress</em>, 2024
          <br>
          <a href="https://doi.org/10.1061/9780784485262.084">Conference paper</a>
          <br><br>
          <p>AIConstruct system is presented to demonstrate, for the first time, how the integration of text and image can create seamless data synchronization for construction progress monitoring and automated schedule generation, unlocking a new research paradigm.</p>
        </div>
      </div>   

      <div class="container2">
        <div class="column-img">
          <div class="video-container">
            <video width="100%" height="100%" muted autoplay loop style="position: absolute; top: 0; left: 0; opacity: 1; z-index: -1;">
              <img src="images/schedule_health.png" type="video/mp4">
              Your browser does not support the video tag.
            </video>
            <img src="images/schedule_health.png" style="width: 100%; display: block;">
          </div>
        </div>
        <div class="column-text">
          <a href="https://doi.org/10.1061/9780784483978.037">
            <span class="papertitle">Integrated Heuristic and Machine Learning Approach for Schedule Health Monitoring in Construction</span>
          </a>
          <br><br>
          <strong>Yoonhwa Jung</strong>, Fouad Amer, Mani Golparvar-Fard
          <br>
          <em>Construction Research Congress</em>, 2022
          <br>
          <a href="">Journal paper (in preparation)</a>/ 
          <a href="https://doi.org/10.1061/9780784483978.037">Conference paper</a>
          <br><br>
          <p>Building on the predefined rules and heuristics formulated in the Defense Contract Management Agency (DCMA)’s 14 Point Schedule Quality Assessment, this paper explores the feasibility of heuristic-based and deep learning methods to assess a project schedule health from qualitative and quantitative perspectives.</p>
        </div>
      </div> 

      <div class="container2">
        <div class="column-img">
          <div class="organ">
            <h2>Review <br> Paper</h2>
          </div>
        </div>
        <div class="column-text">
          <a href="http://itc.scix.net/paper/w78-2021-paper-043">
            <span class="papertitle">A systematic review on the requirements on BIM maturity and formal representation of sequencing knowledge for automated construction scheduling</span>
          </a>
          <br><br>
          <strong>Yoonhwa Jung</strong>, Fouad Amer, Mani Golparvar-Fard
          <br>
          <em>Proceedings of the 38th International Conference of CIB W78, Luxembourg</em>, 2021
          <br>
          <a href="http://itc.scix.net/paper/w78-2021-paper-043">Conference paper</a>
          <br><br>
          <p>A close examination on the problems underpinning construction scheduling theory and practice such as sequencing logic and activity description by offering a systematic review on: 1) the way in which BIM-driven schedules are formalized; and 2) the challenges of tying in Building Information Modeling (BIM) with project schedules and/or BIM-driven schedule creation techniques.</p>
        </div>
      </div>
      <br>   
      <h3>Computer Science domain papers</h3>
      <div class="container2">
        <div class="column-img">
          <div class="video-container">
            <video width="100%" height="100%" muted autoplay loop style="position: absolute; top: 0; left: 0; opacity: 1; z-index: -1;">
              <source src="images/sir_absc.mp4" type="video/mp4">
              Your browser does not support the video tag.
            </video>
            <img src="images/sir_absc_cover.png" style="width: 100%; display: block;">
          </div>
        </div>
        <div class="column-text">
          <a href="https://aclanthology.org/2023.findings-emnlp.572/">
            <span class="papertitle">SIR-ABSC: Incorporating Syntax into RoBERTa-based Sentiment Analysis Models with a Special Aggregator Token</span>
          </a>
          <br><br>
          Ikhyun Cho, <strong>Yoonhwa Jung</strong>, Julia Hockenmaier
          <br>
          <em>EMNLP Findings</em>, 2023
          <br>
          <a href="https://github.com/ihcho2/SIR-ABSC">project page</a>/
          <a href="https://aclanthology.org/2023.findings-emnlp.572.pdf">Paper</a>
          <br><br>
          <p>We present a simple, but effective method to incorporate syntactic dependency information directly into transformer-based language models (e.g. RoBERTa) for Aspect-Based Sentiment Classification (ABSC). Yet, SIR-ABSC outperforms these more complex models, yielding new state-of-the-art results on ABSC.</p>
        </div>
      </div>   

      <div class="container2">
        <div class="column-img">
          <div class="video-container">
            <img src="images/ptmoecap_concealed.png" data-hover="images/ptmoecap_concealed.png">
          </div>
        </div>
        <div class="column-text">
          <a href="https://yifanjiang19.github.io/alignerf">
            <span class="papertitle">Prompting for Mixture-of-Experts: A Prompt-based Mixture-of-Experts framework for Stylized Image Captioning</span>
          </a>
          <br><br>
          Ikhyun Cho, <strong>Yoonhwa Jung</strong>, Julia Hockenmaier
          <br>
          <em>ECCV</em>, 2024, Under review
          <br>
          <a href=""></a>
          <br>
          <p>We introduce PTMoE-Cap, a simple yet effective approach for generating stylized image captions, leveraging the synergy of Mixture-of-Experts (MoE) and prompt learning techniques as a effective routing source.</p>
        </div>
      </div>
            
      <div class="container2 highlight">
        <div class="organ">
            <h2>Machine <br> Unlearning</h2>
          </div>
        <div class="column-text">
          <a href="https://github.com/joonv2/ARU">
            <span class="papertitle">Attack and Reset for Unlearning: Exploiting Adversarial Noise toward Machine Unlearning through Parameter Re-initialization</span>
          </a>
          <br><br>
          <strong>Yoonhwa Jung</strong>, Ikhyun Cho, Shun-Hsiang Hsu, Julia Hockenmaier
          <br>
          <em>Preparing a submission</em>, 2025
          <br>
          <a href="https://github.com/joonv2/ARU">project page</a>/
          <a href="https://arxiv.org/abs/2401.08998">arXiv</a>
          <br><br>
          <p>We leverage meticulously crafted adversarial noise to generate a parameter mask, effectively resetting certain parameters and rendering them unlearnable. A novel approach called Attack-and-Reset for Unlearning (ARU) outperforms current state-of-the-art results on two facial machine-unlearning benchmark datasets.</p>
        </div>
      </div>
    </div>

    <div class="section">
      <h2>Teaching</h2>                     
      <div class="container2">
        <div class="column-img">
          <div class="teaching-img">
          <img src="images/teach.jpg">
          </div>
        </div>
        <div class="left-text">
          <strong><a href="">UIUC CEE 320 - Construction Engineering</a></strong>
          <br>
          Teaching Assistant (Leader TA), Fall 2023
          <br> ㄴ (selected one of the best courses from student evaluation)
          <br>
          Teaching Assistant, Fall 2021, Spring 2022, Fall 2022, Spring 2023
          <br>
          <br>
          <strong>Summer High School Science Program</strong>
          <br>
          Teacher, Mirim Girls’ Info. Sci. High School, Seoul, Jul. 2015
        </div>
      </div>
    </div>


    <div class="section">
      <h2>Reviewer & Organization</h2> 
      <div class="PA-p">
        <p>
          Reviewer | ASCE Journal of Construction Engineering and Management | 2024 – Present
        </p>
        <p>
          Reviewer | Elsevier Automation in Construction | 2024 – Present
        </p>
        <p>
          Reviewer | ASCE Journal of Computing in Civil Engineering | 2023 – Present
        </p>
        <p>
          Member | Association for Computational Linguistics | 2023 – Present
        </p>
        <p>
          Student member | ASCE Data Sensing and Analysis Committee  | 2023 – Present
        </p> 
        <p>
          Student member | ASCE Visualization, Information Modeling, and Simulation | 2023 – Present 
        </p> 
        <p>
          Student member | American Society of Civil Engineers (ASCE) 2022 – Present
        </p>
        <p>
          Student member | Korean Society of Civil Engineers (KSCE) | 2018 – 2019
        </p>
        <p>
          Director of Students’ Association Marketing Depart. | Hanyang Univ. | Mar. 2015 – Dec 2016
        </p>
      </div>     
    </div>
    <div class="section">
      <h2>Affiliations</h2> 
    <table align="center" >
        <tbody><tr>
            <td width="15%" align="center">
                <a href="https://www.documentcrunch.com/" target="_blank">
                <img style="width:80px"  src="images/DC.png" ></a>&nbsp 
            </td>
            
            <td width="15%" align="center">
                <a href="https://raamac.cee.illinois.edu/team-2023" target="_blank">
                <img style="width:80px"  src="images/raamac.png"></a>&nbsp 
            </td>
            <td width="15%" align="center">
                <a href="https://cs.illinois.edu/" target="_blank">
                <img style="width:80px"  src="images/uiuccs.jpg"></a>&nbsp 
            </td>

            <td width="15%" align="center">
                <a href="https://cee.illinois.edu/" target="_blank">
                <img style="width:80px" src="images/uiuccee.png"></a>&nbsp 
            </td> 
            <td width="15%" align="center">
                <a href="https://www.hanyang.ac.kr/web/eng/home" target="_blank">
                <img style="width:80px" src="images/hanyang.png"></a>&nbsp 
            </td> 
        </tr>
        <tr>
            <td width="18%" align="center"><font size="2">Document Crunch<br>2024 - present</font></td>
            <td width="18%" align="center"><font size="2">RAAMAC AI Lab<br>2019 - present</font></td>
            <td width="18%" align="center"><font size="2">UIUC CS NLP Group<br>2022 - 2023 </font></td>
            <td width="18%" align="center"><font size="2">UIUC CEE <br>2019 - present</font></td> 
            <td width="18%" align="center"><font size="2">Hanyang Univ.<br>2015 - 2019</font></td> 
        </tr>
    </tbody></table> 
    </div>
    <footer>
       template credits: <a href="https://jonbarron.info/">this</a> and <a
                      href=" https://djr2015.github.io/">this</a>.
      image credits: Yoonhwa Jung, <a href="https://firefly.adobe.com/">ai-generated</a>.
                 
    </footer>

    <script>
      document.querySelectorAll('.video-container').forEach(function(container) {
        const video = container.querySelector('video');
        const img = container.querySelector('img');
        const originalSrc = img.src;
        const hoverSrc = img.getAttribute('data-hover'); // 호버 시 바꿀 이미지 경로
  
        img.addEventListener('mouseenter', function() {
          if (video) {
            img.style.opacity = '0';
            video.style.opacity = '1';
            video.play();
          } else if (hoverSrc) {
            img.src = hoverSrc; // 이미지 교체
          }
        });
  
        img.addEventListener('mouseleave', function() {
          if (video) {
            video.pause();
            video.currentTime = 0;
            img.style.opacity = '1';
            video.style.opacity = '0';
          } else {
            img.src = originalSrc; // 원래 이미지로 복원
          }
        });
      });
    </script>
  </body>

</html>
